{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam = cv2.VideoCapture(-1)\n",
    "cam.open(0)\n",
    "\n",
    "detector=cv2.CascadeClassifier('face.xml')\n",
    "i=0\n",
    "offset=50\n",
    "type(cam)\n",
    "print(cam.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your id2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name=input('enter your id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(cam.isOpened()):\n",
    "    ret, im =cam.read()\n",
    "    path = '/home/kshea/Projects/Drone/python-ardrone/dataset'\n",
    "    \n",
    "    \n",
    "   \n",
    "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    faces=detector.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(100, 100), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for(x,y,w,h) in faces:\n",
    "        i=i+1\n",
    "        cv2.imwrite(os.path.join(path , \"face-\"+name +'.'+ str(i) + \".jpg\"), gray[y-offset:y+h+offset,x-offset:x+w+offset])\n",
    "        cv2.rectangle(im,(x-50,y-50),(x+w+50,y+h+50),(225,0,0),2)\n",
    "        cv2.imshow('im',im[y-offset:y+h+offset,x-offset:x+w+offset])\n",
    "        cv2.waitKey(1000)\n",
    "    if i>50:\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce55eb852c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Stop the camera\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cam = cv2.VideoCapture(-1)\n",
    "cam.set(3,1296)\n",
    "cam.set(4,730)\n",
    "res, img = cam.read()\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "# Stop the camera\n",
    "cam.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.30.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.34.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.36.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.31.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.20.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.18.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.21.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.38.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.15.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.35.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.45.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.16.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.19.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.17.jpg 1\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.29.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.39.jpg 2\n",
      "/home/kshea/Projects/Drone/python-ardrone/dataset/face-2.37.jpg 2\n"
     ]
    }
   ],
   "source": [
    "import cv2, os\n",
    "\n",
    "# Import numpy for matrix calculation\n",
    "import numpy as np\n",
    "\n",
    "# Import Python Image Library (PIL)\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "def assure_path_exists(path):\n",
    "    dir = os.path.dirname(path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# Create Local Binary Patterns Histograms for face recognization\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "# Using prebuilt frontal face training model, for face detection\n",
    "detector = cv2.CascadeClassifier(\"face.xml\")\n",
    "path = '/home/kshea/Projects/Drone/python-ardrone/dataset'\n",
    "# Create method to get the images and label data\n",
    "def getImagesAndLabels(path):\n",
    "    # Get all file path\n",
    "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)] \n",
    "    \n",
    "    # Initialize empty face sample\n",
    "    faceSamples=[]\n",
    "    \n",
    "    # Initialize empty id\n",
    "    ids = []\n",
    "\n",
    "    # Loop all the file path\n",
    "    for imagePath in imagePaths:\n",
    "\n",
    "        # Get the image and convert it to grayscale\n",
    "        PIL_img = Image.open(imagePath).convert('L')\n",
    "\n",
    "        # PIL image to numpy array\n",
    "        img_numpy = np.array(PIL_img,'uint8')\n",
    "\n",
    "        # Get the image id\n",
    "        id = int(os.path.split(imagePath)[1].split(\".\")[0].replace(\"face-\", \"\"))\n",
    "        print(imagePath,id)\n",
    "        # Get the face from the training images\n",
    "        faces = detector.detectMultiScale(img_numpy)\n",
    "\n",
    "        # Loop for each face, append to their respective ID\n",
    "        for (x,y,w,h) in faces:\n",
    "\n",
    "            # Add the image to face samples\n",
    "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
    "\n",
    "            # Add the ID to IDs\n",
    "            ids.append(id)\n",
    "\n",
    "    # Pass the face array and IDs array\n",
    "    return faceSamples,ids\n",
    "\n",
    "# Get the faces and IDs\n",
    "faces,ids = getImagesAndLabels(path)\n",
    "\n",
    "# Train the model using the faces and IDs\n",
    "recognizer.train(faces, np.array(ids))\n",
    "\n",
    "# Save the model into trainer.yml\n",
    "assure_path_exists('trainer/')\n",
    "recognizer.save('trainer/trainer.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getImagesAndLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5f809e62558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetImagesAndLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'getImagesAndLabels' is not defined"
     ]
    }
   ],
   "source": [
    "faces,ids = getImagesAndLabels('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.11.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.51.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.23.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.35.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.49.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.28.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.38.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.20.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.18.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.21.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.13.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.26.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.42.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.15.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.36.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.33.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.8.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.24.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.2.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.25.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.44.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.40.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.43.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.12.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.34.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.1.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.27.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.29.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.22.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.5.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.30.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.50.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.14.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.16.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.41.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.4.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.19.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.47.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.9.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.17.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.32.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.6.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.46.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.52.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.10.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.3.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.37.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.39.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.31.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.7.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.48.jpg', '/home/kshea/Projects/Drone/python-ardrone/dataset/face-1.45.jpg']\n"
     ]
    }
   ],
   "source": [
    "path = '/home/kshea/Projects/Drone/python-ardrone/dataset'\n",
    "  \n",
    "print( [os.path.join(path,f) for f in os.listdir(path)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import numpy for matrices calculations\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2,numpy,os\n",
    "import os \n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "\n",
    "def assure_path_exists(path):\n",
    "    dir = os.path.dirname(path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# Create Local Binary Patterns Histograms for face recognization\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "assure_path_exists(\"trainer/\")\n",
    "\n",
    "# Load the trained mode\n",
    "recognizer.read('trainer/trainer.yml')\n",
    "\n",
    "# Load prebuilt model for Frontal Face\n",
    "cascadePath = \"face.xml\"\n",
    "\n",
    "# Create classifier from prebuilt model\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "from twilio.rest import Client\n",
    "# Set the font style\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Initialize and start the video frame capture\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read the video frame\n",
    "    ret, im =cam.read()\n",
    "\n",
    "    # Convert the captured frame into grayscale\n",
    "    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get all face from the video frame\n",
    "    faces=faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(100, 100), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # For each face in faces\n",
    "    for(x,y,w,h) in faces:\n",
    "\n",
    "        # Create rectangle around the face\n",
    "        cv2.rectangle(im, (x-20,y-20), (x+w+20,y+h+20), (0,255,0), 4)\n",
    "\n",
    "        # Recognize the face belongs to which ID\n",
    "        Id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        #print(Id)\n",
    "        # Check the ID if exist \n",
    "        if(Id == 1):\n",
    "            Id = \"harsha {0:.2f}%\".format(round(100 - confidence, 2))\n",
    "        elif(Id == 2):\n",
    "            Id = \"b {0:.2f}%\".format(round(100 - confidence, 2))\n",
    "\n",
    "        # Put text describe who is in the picture\n",
    "        cv2.rectangle(im, (x-22,y-90), (x+w+22, y-22), (0,255,0), -1)\n",
    "        cv2.putText(im, str(Id), (x,y-40), font, 1, (255,255,255), 3)\n",
    "\n",
    "    # Display the video frame with the bounded rectangle\n",
    "    cv2.imshow('im',im)  \n",
    "\n",
    "    # If 'q' is pressed, close program\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Stop the camera\n",
    "cam.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2,numpy,os\n",
    "#import parse_sensor_data \n",
    "import ardrone\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#from twilio.rest import Client\n",
    "\n",
    "\n",
    "# Create Local Binary Patterns Histograms for face recognization\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "assure_path_exists(\"trainer/\")\n",
    "\n",
    "# Load the trained mode\n",
    "recognizer.read('trainer/trainer.yml')\n",
    "\n",
    "# Load prebuilt model for Frontal Face\n",
    "cascadePath = \"face.xml\"\n",
    "\n",
    "# Create classifier from prebuilt model\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "\n",
    "# Set the font style\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream and resize it\n",
    "    # to have a maximum width of 400 pixels\n",
    "    \n",
    "    #frame=vs.read()\n",
    "    ret, im =cam.read()\n",
    "\n",
    "    frame = im\n",
    "    \n",
    "    open_cv_image =numpy.array(frame)\n",
    "    \n",
    "    frame =open_cv_image[:,:,::-1].copy()\n",
    "    im=frame# Convert the captured frame into grayscale\n",
    "    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get all face from the video frame\n",
    "    faces=faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(100, 100), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    frame = imutils.resize(frame, width=650)\n",
    "\n",
    "    # grab the frame dimensions and convert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (650, 650)),0.007843, (650, 650), 127.5)\n",
    "\n",
    "    # pass the blob through the network and obtain the detections and\n",
    "    # predictions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # extract the index of the class label from the\n",
    "            # `detections`, then compute the (x, y)-coordinates of\n",
    "            # the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx],confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    for(x,y,w,h) in faces:\n",
    "            # Create rectangle around the face\n",
    "            cv2.rectangle(im, (x-20,y-20), (x+w+20,y+h+20), (0,255,0), 4)\n",
    "\n",
    "        # Recognize the face belongs to which ID\n",
    "            Id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "            print(Id)\n",
    "            # Check the ID if exist \n",
    "            if(Id == 1):\n",
    "                Id = \"harsha ,Male,chicago {0:.2f}%\".format(round(100 - confidence, 2))\n",
    "            elif(Id == 2):\n",
    "                Id = \"jonathan,Male Urbana champaign{0:.2f}%\".format(round(100 - confidence, 2))\n",
    "            # Put text describe who is in the picture\n",
    "            cv2.rectangle(im, (x-22,y-90), (x+w+22, y-22), (0,255,0), -1)\n",
    "            cv2.putText(im, str(Id), (x,y-40), font, 1, (255,255,255), 3)\n",
    "\n",
    "    cv2.imshow(\"Person \", im)\n",
    "    key = cv2.waitKey(100) & 0xFF\n",
    "    \n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    fps.update()\n",
    "\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-865d64bf38b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfaceCascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, im =cam.read()\n",
    "    gray=cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "    faces=faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(100, 100), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for(x,y,w,h) in faces:\n",
    "        nbr_predicted, conf = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        cv2.rectangle(im,(x-50,y-50),(x+w+50,y+h+50),(225,0,0),2)\n",
    "        if(nbr_predicted==7):\n",
    "             nbr_predicted='Obama'\n",
    "        elif(nbr_predicted==2):\n",
    "             nbr_predicted='Anirban'\n",
    "        cv2.PutText(cv2.cv.fromarray(im),str(nbr_predicted)+\"--\"+str(conf), (x,y+h),font, 255) #Draw the text\n",
    "        cv2.imshow('im',im)\n",
    "cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ardrone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8f55f5da1cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mardrone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdrone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mardrone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARDrone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ardrone'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import ardrone\n",
    "drone = ardrone.ARDrone()\n",
    "from PIL import Image\n",
    "\n",
    "def twillo():\n",
    "    from twilio.rest import Client\n",
    "\n",
    "\n",
    "    # Your Account Sid and Auth Token from twilio.com/console\n",
    "    account_sid = 'AC12446ff63d16f5749cf9d3d59c0465dc'\n",
    "    auth_token = '77672a18715f646025afea018ef1094d'\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    message = client.messages \\\n",
    "                    .create(\n",
    "                         body=\"harsha, Male chicago \",\n",
    "                         from_='+1(312) 471-0558',\n",
    "                         to='+13123588613'\n",
    "                     )\n",
    "\n",
    "    print(message.sid)\n",
    "    \n",
    "\n",
    "# Import numpy for matrices calculations\n",
    "\n",
    "def assure_path_exists(path):\n",
    "    dir = os.path.dirname(path)\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# Create Local Binary Patterns Histograms for face recognization\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "assure_path_exists(\"trainer/\")\n",
    "\n",
    "# Load the trained mode\n",
    "recognizer.read('trainer/trainer.yml')\n",
    "\n",
    "# Load prebuilt model for Frontal Face\n",
    "cascadePath = \"face.xml\"\n",
    "\n",
    "# Create classifier from prebuilt model\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "\n",
    "# Set the font style\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "x=1\n",
    "# Loop\n",
    "while True:\n",
    "    # Read the video frame\n",
    "    frame = drone.image\n",
    "    open_cv_image =numpy.array(frame)\n",
    "\n",
    "    im =open_cv_image[:,:,::-1].copy()\n",
    "\n",
    "    # Convert the captured frame into grayscale\n",
    "    gray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get all face from the video frame\n",
    "    faces=faceCascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(100, 100), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # For each face in faces\n",
    "    for(x,y,w,h) in faces:\n",
    "\n",
    "        # Create rectangle around the face\n",
    "        cv2.rectangle(im, (x-20,y-20), (x+w+20,y+h+20), (0,255,0), 4)\n",
    "\n",
    "        # Recognize the face belongs to which ID\n",
    "        Id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        print(Id)\n",
    "        # Check the ID if exist \n",
    "        if(Id == 1):\n",
    "            print(\"hi\")\n",
    "            if(x==1):\n",
    "                twillo()\n",
    "                x=2\n",
    "            Id = \"harsha {0:.2f}%\".format(round(100 - confidence, 2))\n",
    "        elif(Id == 2):\n",
    "            print(\"hi\")\n",
    "            Id = \"b {0:.2f}%\".format(round(100 - confidence, 2))\n",
    "\n",
    "        # Put text describe who is in the picture\n",
    "        cv2.rectangle(im, (x-22,y-90), (x+w+22, y-22), (0,255,0), -1)\n",
    "        cv2.putText(im, str(Id), (x,y-40), font, 1, (255,255,255), 3)\n",
    "\n",
    "    # Display the video frame with the bounded rectangle\n",
    "    cv2.imshow('im',im) \n",
    "    cv2.imshow('im2',im) \n",
    "\n",
    "    # If 'q' is pressed, close program\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "drone.halt()\n",
    "# Stop the camera\n",
    "cam.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
